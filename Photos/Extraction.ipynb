{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on how to use the algorithm :\n",
    "Create a main folder. Inside the main folder create a folder called **form-samples**. Put all the images in form-samples folder. Put this code in the main folder and run it. This creates another folder called **crop** inside the main folder and puts all the extracted feature images of the first file in form-samples to **crop1** folder, all the extracted feature images of the second file in form-samples to **crop2** folder and so on. All the crop*k*(crop1, crop2 etc.) folders are inside the crop folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.60194730758667 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "print(time.time()-start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def order_points(pts,crop,main=0):\n",
    "    ''' \n",
    "        The function returns the 4 corner points in a set of points and returns the rectangle formed by the four\n",
    "        co-ordinates. \n",
    "        \n",
    "        The input are a set of points (pts).\n",
    "        \n",
    "        'crop' is the amount of pixels that are subtracted from the points obtained which is kind of padding \n",
    "        so that we don't lose the data that run outside the area specified.\n",
    "        \n",
    "        'main' is the flag which exclusively runs when the ordering points are of the main form. If main is 1, then\n",
    "        the cropping should be more (This is specific to the form that we have chosen)\n",
    "    '''\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    " \n",
    "\n",
    "    s = pts.sum(axis = 1)\n",
    "\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    " \n",
    "\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    \n",
    "    if main:\n",
    "        rect[1][1]-=110\n",
    "        rect[0][0] -=20\n",
    "    \n",
    "    #cropping a little more towards left\n",
    "    rect[0][0] -=crop\n",
    "    rect[3][0] -=crop\n",
    "    #cropping a little more towards right\n",
    "    rect[1][0] +=crop\n",
    "    rect[2][0] +=crop\n",
    "    #cropping a little more towards bottom\n",
    "    rect[3][1] +=crop\n",
    "    rect[2][1] +=crop\n",
    " \n",
    "    return rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_outer_points(pts,crop):\n",
    "    ''' \n",
    "        The function returns the 4 corner points in a set of points but in a different way and returns the rectrangle \n",
    "        formed from the four co-ordinates. \n",
    "        \n",
    "        The input are a set of points (pts).\n",
    "        \n",
    "        'crop' is the amount of pixels that are subtracted from the four points obtained which is kind of padding \n",
    "        so that we don't lose the data that run outside the area specified.\n",
    "        The top left is minimum_x coordinate and minimum y-cordinate.\n",
    "        The top right is maximum_x coordinate and minimum y-cordinate and so on.\n",
    "    '''\n",
    "\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    " \n",
    "    min_x = min(pts[:,0])\n",
    "    min_y = min(pts[:,1])\n",
    "    max_x = max(pts[:,0])\n",
    "    max_y = max(pts[:,1])\n",
    "\n",
    "    rect[0][0] = min_x\n",
    "    rect[0][1] = min_y\n",
    "    rect[2][0] = max_x\n",
    "    rect[2][1] = max_y\n",
    " \n",
    "\n",
    "    rect[1][0] = max_x\n",
    "    rect[1][1] = min_y\n",
    "    rect[3][0] = min_x\n",
    "    rect[3][1] = max_y\n",
    "    \n",
    "    #cropping a little more towards left\n",
    "    rect[0][0] -=crop\n",
    "    rect[3][0] -=crop\n",
    "    #cropping a little more towards right\n",
    "    rect[1][0] +=crop\n",
    "    rect[2][0] +=crop\n",
    "    #cropping a little more towards bottom\n",
    "    rect[3][1] +=crop\n",
    "    rect[2][1] +=crop\n",
    " \n",
    "    return rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def four_point_warp(image, pts,crop,outer=1,main=0):\n",
    "    ''' \n",
    "        The function finds the corner four points of the image and warps the image accordingly.\n",
    "        \n",
    "        image : The grayscale image of the scanned form.\n",
    "        \n",
    "        pts   : The set of points from which corner points are to be selected to warp.\n",
    "        \n",
    "        crop  : The buffer amount of pixels that should be cropped less so that we don't lose the data.\n",
    "        \n",
    "        outer : If outer is 1 get_outer_points() function is called, else order_points() function is called.\n",
    "        \n",
    "        main  : This is the variable for the function get_outer_points().\n",
    "    '''\n",
    "    if outer:\n",
    "        rect = get_outer_points(pts,crop)\n",
    "    else :\n",
    "        rect = order_points(pts,crop,main)\n",
    "    (tl, tr, br, bl) = rect\n",
    "    \n",
    "    # Finding width and height of the image based on corner points\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "    \n",
    "    # The destination where the image needs to be pasted. \n",
    "    # This is the size of the image where the warped image will be put.\n",
    "    dst = np.array([\n",
    "              [0, 0],\n",
    "              [maxWidth - 1, 0],\n",
    "              [maxWidth - 1, maxHeight - 1],\n",
    "              [0, maxHeight - 1]], dtype = \"float32\")\n",
    "    \n",
    "    # Warping the image\n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    " \n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_features_orb(gray_main, gray_crop, mask_tl=None, mask_br=None):\n",
    "    '''\n",
    "        This function is used to compare the features of two images to match them based on the features detected.\n",
    "        It detects the features and returns all the good points of the features based on the Lowe's ratio test.\n",
    "        It uses ORB (Oriented FAST and Rotated BRIEF) algorithm to compare features.\n",
    "        \n",
    "        gray_main : The gray-scale image that is to be compared for the features.\n",
    "        \n",
    "        gray_crop : The gray-scale image of the standard image with features that is used to compare other images.\n",
    "        \n",
    "        The next two arguments are to reduce the computation time\n",
    "        mask_tl   : This is the top left co-ordinates of a mask that covers the approximate central part of the image \n",
    "                    so that it doesn't match features there since we need only the corner points.\n",
    "        \n",
    "        mask_br   : This is the bottom right co-ordinates of a mask that covers the approximate central part of the image \n",
    "                    so that it doesn't match features there since we need only the corner points.\n",
    "        \n",
    "    '''\n",
    "    #Trying feature matching using ORB\n",
    "    orb = cv2.ORB_create()\n",
    "    Mask = None\n",
    "\n",
    "    if mask_tl and mask_br:\n",
    "        Mask = np.zeros(gray_main.shape[:2], dtype=np.uint8)\n",
    "        cv2.rectangle(Mask, mask_tl, mask_br, (255), thickness = -1)\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = orb.detectAndCompute(gray_main,mask)\n",
    "    kp2, des2 = orb.detectAndCompute(gray_crop,None)\n",
    "    \n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "    matches = bf.match(des1,des2)\n",
    "\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "    img = cv2.drawMatches(gray_main,kp1,gray_crop,kp2,matches,None, flags=2)\n",
    "\n",
    "    return np.float32([ kp1[m.queryIdx].pt for m in matches ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_features(gray_main, gray_crop, mask_tl=None, mask_br=None, Mask_main=None):\n",
    "    '''\n",
    "        This function is used to compare the features of two images to match them based on the features detected.\n",
    "        It detects the features and returns all the good points of the features based on the Lowe's ratio test.\n",
    "        It uses Scale-Invariant Feature Transform algorithm to compare features.\n",
    "        \n",
    "        gray_main : The gray-scale image that is to be compared for the features.\n",
    "        \n",
    "        gray_crop : The gray-scale image of the standard image with features that is used to compare other images.\n",
    "        \n",
    "        The next two arguments are to reduce the computation time\n",
    "        mask_tl   : This is the top left co-ordinates of a mask that covers the approximate central part of the image \n",
    "                    so that it doesn't match features there since we need only the corner points.\n",
    "        \n",
    "        mask_br   : This is the bottom right co-ordinates of a mask that covers the approximate central part of the image \n",
    "                    so that it doesn't match features there since we need only the corner points.\n",
    "        \n",
    "        Mask_main : Mask when the main form is to be compared\n",
    "    '''\n",
    "    #Trying feature matching using SIFT\n",
    "    # Initiate SIFT detector\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    \n",
    "    Mask=Mask_main\n",
    "    \n",
    "    if mask_tl and mask_br:\n",
    "        if Mask_main is not None:\n",
    "            cv2.rectangle(Mask, mask_tl, mask_br, (0), thickness = -1)\n",
    "        else :\n",
    "            Mask = np.zeros(gray_main.shape[:2], dtype=np.uint8)\n",
    "            cv2.rectangle(Mask, mask_tl, mask_br, (255), thickness = -1)\n",
    "    \n",
    "    \n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(gray_main,Mask)\n",
    "    kp2, des2 = sift.detectAndCompute(gray_crop,None)\n",
    "    \n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks = 50)\n",
    "    \n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    \n",
    "    matches = flann.knnMatch(des1,des2,k=2,mask=Mask)\n",
    "    \n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.8*n.distance:\n",
    "            good.append(m)\n",
    "    \n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    \n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    matchesMask = mask.ravel().tolist()\n",
    "    \n",
    "    h,w = gray_main.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts,M)\n",
    "    \n",
    "    \n",
    "    img2 = cv2.polylines(gray_crop,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "    \n",
    "    draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 2)\n",
    "\n",
    "    img3 = cv2.drawMatches(gray_main,kp1,gray_crop,kp2,good,None,**draw_params)\n",
    "    return np.float32([ kp1[m.queryIdx].pt for m in good ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_input(main_file, crop_file):\n",
    "    '''\n",
    "        This function reads the input(main_file is the scanned image and crop_file is the standard image used \n",
    "         for comparision) and returns the required output.\n",
    "    '''\n",
    "    img1 = cv2.imread(main_file)\n",
    "    crop = cv2.imread(crop_file)\n",
    "    gray_main = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "    gray_crop = cv2.cvtColor(crop,cv2.COLOR_BGR2GRAY)\n",
    "    height_main, width_main = gray_main.shape\n",
    "    height_crop, width_crop = gray_crop.shape\n",
    "    height = height_main\n",
    "    width = width_main\n",
    "\n",
    "    return img1, gray_main, gray_crop, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop_selected_rectangle(img, tl, br):\n",
    "    '''\n",
    "        Given top left(tl) and bottom right (br) x and y co-ordinates of the image(img) returns the cropped image of the \n",
    "        co-ordinates.\n",
    "    '''\n",
    "    return img[tl[1]:br[1],tl[0]:br[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filled-pa.jpg\n",
      "Saving into crop1\n",
      "Time taken :  87.8336250782013\n",
      "filled-s1.jpg\n",
      "Saving into crop2\n",
      "Time taken :  156.4682605266571\n",
      "filled-s2.jpg\n",
      "Saving into crop3\n",
      "Time taken :  219.932514667511\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    This is the main code snippet that calls all the required functions and extracts the features and stores them.\n",
    "'''\n",
    "start_time = time.time()\n",
    "\n",
    "types = ('*.jpg', '*.JPG','*.JPEG', '*.jpeg')\n",
    "files = []\n",
    "os.system(\"mkdir crop\")\n",
    "os.chdir(os.getcwd() + \"/form-samples/\")\n",
    "\n",
    "# Takes all the files in form-samples\n",
    "for t in types:\n",
    "    if glob.glob(t) != [] :\n",
    "        files.append(glob.glob(t))\n",
    "\n",
    "# Creates crop folders\n",
    "os.chdir(\"../crop/\")\n",
    "i=1\n",
    "for f in files[0]:\n",
    "    print(f)\n",
    "    # Reading input\n",
    "    img1, gray_main, gray_crop, height, width = read_input('../form-samples/'+f, '../form_print.jpg')\n",
    "    mask = np.zeros(gray_main.shape[:2], dtype=np.uint8)+255\n",
    "    \n",
    "    # Get all the feature points after comparison\n",
    "    points = compare_features(gray_main, gray_crop, (0,int(height/4)),(width,int(18*height/25)),mask)\n",
    "    \n",
    "    crop = 30\n",
    "    # Warp the image based on the points obtained after feature extraction.\n",
    "    dst = four_point_warp(gray_main, points, crop, outer=0, main=1)\n",
    "    height, width = dst.shape\n",
    "    \n",
    "    os.system(\"mkdir crop\"+str(i))\n",
    "    # Crop the required fields like branch_name, date, Account number.\n",
    "    branch = crop_selected_rectangle(dst,(int(width/20),int(height/20)),(int(7*width/11),int(3*height/37)))\n",
    "    date = crop_selected_rectangle(dst,(int(25*width/37),int(height/21)),(int(width),int(3*height/37)))\n",
    "    ac_no = crop_selected_rectangle(dst,(0,int(height/12)),(int(23*width/44),int(19*height/160)))\n",
    "    name = crop_selected_rectangle(dst,(int(width/70),int(37*height/192)),(int(31*width/34),int(11*height/50)))\n",
    "    print('Saving into crop'+str(i))\n",
    "    os.chdir(os.getcwd()+'/crop'+str(i))\n",
    "    \n",
    "    # Writing everything to files\n",
    "    cv2.imwrite('branch.jpg',branch)\n",
    "    cv2.imwrite('Date.jpg',date)\n",
    "    cv2.imwrite('Ac_no.jpg',ac_no)\n",
    "    cv2.imwrite('Name.jpg',name)\n",
    "    os.chdir('../')\n",
    "    print('Time taken : ',time.time()-start_time)\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The rest of the cells are for coder reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3742.9271965026855\n",
      "2902 2062\n"
     ]
    }
   ],
   "source": [
    "# crop = 30\n",
    "# dst = four_point_warp(gray_main, points, crop, outer=0, main=1)\n",
    "# print(time.time()-start_time)\n",
    "# height, width = dst.shape\n",
    "# print(height,width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crop branch\n",
    "# cv2.rectangle(dst,(int(width/20),int(height/20)),(int(7*width/11),int(3*height/37)),(0),3)\n",
    "# branch = crop_selected_rectangle(dst,(int(width/20),int(height/20)),(int(7*width/11),int(3*height/37)))\n",
    "# date\n",
    "# cv2.rectangle(dst,(int(25*width/37),int(height/21)),(int(width),int(3*height/37)),(0),3)\n",
    "# Ac no.\n",
    "# cv2.rectangle(dst,(0,int(height/12)),(int(23*width/44),int(19*height/160)),(0),3)\n",
    "# cv2.rectangle(dst,(int(width/70),int(37*height/192)),(int(31*width/34),int(11*height/50)),(0),3)\n",
    "# cv2.imwrite('Out.jpg',dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/darshan/CS/Summer2018/Impel/OCR/Darshan/Photos/crop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i=0\n",
    "# os.system(\"mkdir crop\")\n",
    "# os.chdir('..')\n",
    "# os.system(\"mkdir crop\"+str(i))\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For residential address\n",
    "# address_x = (int(0),int(18*height/25))\n",
    "# address_y = (width,int(25*height/26))\n",
    "\n",
    "# points = compare_features(gray_main, gray_crop, address_x, address_y)\n",
    "# points = points[points[:,1]>(7*height)/10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
